{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"VGGNet16_models(Tensorflow).ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyO7UGr66n0suzmPEXw4hR0V"},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"code","metadata":{"id":"PDARYlagDFWQ"},"source":["# Copyright 2015 The TensorFlow Authors. All Rights Reserved.\r\n","#\r\n","# Licensed under the Apache License, Version 2.0 (the \"License\");\r\n","# you may not use this file except in compliance with the License.\r\n","# You may obtain a copy of the License at\r\n","#\r\n","#     http://www.apache.org/licenses/LICENSE-2.0\r\n","#\r\n","# Unless required by applicable law or agreed to in writing, software\r\n","# distributed under the License is distributed on an \"AS IS\" BASIS,\r\n","# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\r\n","# See the License for the specific language governing permissions and\r\n","# limitations under the License.\r\n","# ==============================================================================\r\n","# pylint: disable=invalid-name\r\n","\"\"\"VGG16 model for Keras.\r\n","Reference:\r\n","  - [Very Deep Convolutional Networks for Large-Scale Image Recognition]\r\n","    (https://arxiv.org/abs/1409.1556) (ICLR 2015)\r\n","\"\"\"\r\n","from __future__ import absolute_import\r\n","from __future__ import division\r\n","from __future__ import print_function\r\n","\r\n","from tensorflow.python.keras import backend\r\n","from tensorflow.python.keras.applications import imagenet_utils\r\n","from tensorflow.python.keras.engine import training\r\n","from tensorflow.python.keras.layers import VersionAwareLayers\r\n","from tensorflow.python.keras.utils import data_utils\r\n","from tensorflow.python.keras.utils import layer_utils\r\n","from tensorflow.python.lib.io import file_io\r\n","from tensorflow.python.util.tf_export import keras_export\r\n","\r\n","\r\n","WEIGHTS_PATH = ('https://storage.googleapis.com/tensorflow/keras-applications/'\r\n","                'vgg16/vgg16_weights_tf_dim_ordering_tf_kernels.h5')\r\n","WEIGHTS_PATH_NO_TOP = ('https://storage.googleapis.com/tensorflow/'\r\n","                       'keras-applications/vgg16/'\r\n","                       'vgg16_weights_tf_dim_ordering_tf_kernels_notop.h5')\r\n","\r\n","layers = VersionAwareLayers()\r\n","\r\n","#keras.applications.VGG16를 사용하면 호출되는 함수\r\n","@keras_export('keras.applications.vgg16.VGG16', 'keras.applications.VGG16')\r\n","def VGG16(\r\n","    include_top=True, #분류 층 포함할지 여부 \r\n","    weights='imagenet',\r\n","    input_tensor=None,\r\n","    input_shape=None,\r\n","    pooling=None,\r\n","    classes=1000,\r\n","    classifier_activation='softmax'):\r\n","  \"\"\"Instantiates the VGG16 model.\r\n","  Reference:\r\n","  - [Very Deep Convolutional Networks for Large-Scale Image Recognition](\r\n","  https://arxiv.org/abs/1409.1556) (ICLR 2015)\r\n","  By default, it loads weights pre-trained on ImageNet. Check 'weights' for\r\n","  other options.\r\n","  This model can be built both with 'channels_first' data format\r\n","  (channels, height, width) or 'channels_last' data format\r\n","  (height, width, channels).\r\n","  The default input size for this model is 224x224.\r\n","  Note: each Keras Application expects a specific kind of input preprocessing.\r\n","  For VGG16, call `tf.keras.applications.vgg16.preprocess_input` on your\r\n","  inputs before passing them to the model.\r\n","  Arguments:\r\n","      include_top: whether to include the 3 fully-connected\r\n","          layers at the top of the network.\r\n","      weights: one of `None` (random initialization),\r\n","            'imagenet' (pre-training on ImageNet),\r\n","            or the path to the weights file to be loaded.\r\n","      input_tensor: optional Keras tensor\r\n","          (i.e. output of `layers.Input()`)\r\n","          to use as image input for the model.\r\n","      input_shape: optional shape tuple, only to be specified\r\n","          if `include_top` is False (otherwise the input shape\r\n","          has to be `(224, 224, 3)`\r\n","          (with `channels_last` data format)\r\n","          or `(3, 224, 224)` (with `channels_first` data format).\r\n","          It should have exactly 3 input channels,\r\n","          and width and height should be no smaller than 32.\r\n","          E.g. `(200, 200, 3)` would be one valid value.\r\n","      pooling: Optional pooling mode for feature extraction\r\n","          when `include_top` is `False`.\r\n","          - `None` means that the output of the model will be\r\n","              the 4D tensor output of the\r\n","              last convolutional block.\r\n","          - `avg` means that global average pooling\r\n","              will be applied to the output of the\r\n","              last convolutional block, and thus\r\n","              the output of the model will be a 2D tensor.\r\n","          - `max` means that global max pooling will\r\n","              be applied.\r\n","      classes: optional number of classes to classify images\r\n","          into, only to be specified if `include_top` is True, and\r\n","          if no `weights` argument is specified.\r\n","      classifier_activation: A `str` or callable. The activation function to use\r\n","          on the \"top\" layer. Ignored unless `include_top=True`. Set\r\n","          `classifier_activation=None` to return the logits of the \"top\" layer.\r\n","  Returns:\r\n","    A `keras.Model` instance.\r\n","  Raises:\r\n","    ValueError: in case of invalid argument for `weights`,\r\n","      or invalid input shape.\r\n","    ValueError: if `classifier_activation` is not `softmax` or `None` when\r\n","      using a pretrained top layer.\r\n","  \"\"\"\r\n","  if not (weights in {'imagenet', None} or file_io.file_exists_v2(weights)):\r\n","    raise ValueError('The `weights` argument should be either '\r\n","                     '`None` (random initialization), `imagenet` '\r\n","                     '(pre-training on ImageNet), '\r\n","                     'or the path to the weights file to be loaded.')\r\n","\r\n","  if weights == 'imagenet' and include_top and classes != 1000:\r\n","    raise ValueError('If using `weights` as `\"imagenet\"` with `include_top`'\r\n","                     ' as true, `classes` should be 1000')\r\n","  # Determine proper input shape\r\n","  input_shape = imagenet_utils.obtain_input_shape(\r\n","      input_shape,\r\n","      default_size=224,\r\n","      min_size=32,\r\n","      data_format=backend.image_data_format(),\r\n","      require_flatten=include_top,\r\n","      weights=weights)\r\n","\r\n","  if input_tensor is None:\r\n","    img_input = layers.Input(shape=input_shape)\r\n","  else:\r\n","    if not backend.is_keras_tensor(input_tensor):\r\n","      img_input = layers.Input(tensor=input_tensor, shape=input_shape)\r\n","    else:\r\n","      img_input = input_tensor\r\n","\r\n","  # Block 1\r\n","  #layers.Conv2D(filters,kernel_size)\r\n","  #channels : 64, kernel_size : (3,3), activation function = ReLU, padding='same'(padding 사용), name : 해당 layer의 이름 설정\r\n","  x = layers.Conv2D(\r\n","      64, (3, 3), activation='relu', padding='same', name='block1_conv1')(\r\n","          img_input)\r\n","  x = layers.Conv2D(\r\n","      64, (3, 3), activation='relu', padding='same', name='block1_conv2')(x)\r\n","  #pool_size : (2,2)\r\n","  x = layers.MaxPooling2D((2, 2), strides=(2, 2), name='block1_pool')(x)\r\n","\r\n","  # Block 2\r\n","  x = layers.Conv2D(\r\n","      128, (3, 3), activation='relu', padding='same', name='block2_conv1')(x)\r\n","  x = layers.Conv2D(\r\n","      128, (3, 3), activation='relu', padding='same', name='block2_conv2')(x)\r\n","  x = layers.MaxPooling2D((2, 2), strides=(2, 2), name='block2_pool')(x)\r\n","\r\n","  # Block 3\r\n","  x = layers.Conv2D(\r\n","      256, (3, 3), activation='relu', padding='same', name='block3_conv1')(x)\r\n","  x = layers.Conv2D(\r\n","      256, (3, 3), activation='relu', padding='same', name='block3_conv2')(x)\r\n","  x = layers.Conv2D(\r\n","      256, (3, 3), activation='relu', padding='same', name='block3_conv3')(x)\r\n","  x = layers.MaxPooling2D((2, 2), strides=(2, 2), name='block3_pool')(x)\r\n","\r\n","  # Block 4\r\n","  x = layers.Conv2D(\r\n","      512, (3, 3), activation='relu', padding='same', name='block4_conv1')(x)\r\n","  x = layers.Conv2D(\r\n","      512, (3, 3), activation='relu', padding='same', name='block4_conv2')(x)\r\n","  x = layers.Conv2D(\r\n","      512, (3, 3), activation='relu', padding='same', name='block4_conv3')(x)\r\n","  x = layers.MaxPooling2D((2, 2), strides=(2, 2), name='block4_pool')(x)\r\n","\r\n","  # Block 5\r\n","  x = layers.Conv2D(\r\n","      512, (3, 3), activation='relu', padding='same', name='block5_conv1')(x)\r\n","  x = layers.Conv2D(\r\n","      512, (3, 3), activation='relu', padding='same', name='block5_conv2')(x)\r\n","  x = layers.Conv2D(\r\n","      512, (3, 3), activation='relu', padding='same', name='block5_conv3')(x)\r\n","  x = layers.MaxPooling2D((2, 2), strides=(2, 2), name='block5_pool')(x)\r\n","\r\n","  #분류 층을 포함한다면\r\n","  if include_top:\r\n","    # Classification block\r\n","    #Flatten\r\n","    x = layers.Flatten(name='flatten')(x)\r\n","    #Fully connected layer 1\r\n","    x = layers.Dense(4096, activation='relu', name='fc1')(x)\r\n","    #Fully connected layer 2\r\n","    x = layers.Dense(4096, activation='relu', name='fc2')(x)\r\n","\r\n","    imagenet_utils.validate_activation(classifier_activation, weights)\r\n","    #최종 분류층 (activation 보통 softmax로 사용)\r\n","    x = layers.Dense(classes, activation=classifier_activation,\r\n","                     name='predictions')(x)\r\n","  else:\r\n","    if pooling == 'avg':\r\n","      x = layers.GlobalAveragePooling2D()(x)\r\n","    elif pooling == 'max':\r\n","      x = layers.GlobalMaxPooling2D()(x)\r\n","\r\n","  # Ensure that the model takes into account\r\n","  # any potential predecessors of `input_tensor`.\r\n","  if input_tensor is not None:\r\n","    inputs = layer_utils.get_source_inputs(input_tensor)\r\n","  else:\r\n","    inputs = img_input\r\n","  # Create model.\r\n","  model = training.Model(inputs, x, name='vgg16')\r\n","\r\n","  # Load weights.\r\n","  #ImageNet weight 사용할 경우 \r\n","  if weights == 'imagenet':\r\n","    #분류 층 포함 인 경우 \r\n","    if include_top:\r\n","      weights_path = data_utils.get_file(\r\n","          'vgg16_weights_tf_dim_ordering_tf_kernels.h5',\r\n","          WEIGHTS_PATH,\r\n","          cache_subdir='models',\r\n","          file_hash='64373286793e3c8b2b4e3219cbf3544b')\r\n","    #특징 분류 부분만 \r\n","    else:\r\n","      weights_path = data_utils.get_file(\r\n","          'vgg16_weights_tf_dim_ordering_tf_kernels_notop.h5',\r\n","          WEIGHTS_PATH_NO_TOP,\r\n","          cache_subdir='models',\r\n","          file_hash='6d6bbae143d832006294945121d1f1fc')\r\n","    #모델에 가중치 로드\r\n","    model.load_weights(weights_path)\r\n","  elif weights is not None:\r\n","    model.load_weights(weights)\r\n","\r\n","  return model\r\n","\r\n","\r\n","@keras_export('keras.applications.vgg16.preprocess_input')\r\n","def preprocess_input(x, data_format=None):\r\n","  return imagenet_utils.preprocess_input(\r\n","      x, data_format=data_format, mode='caffe')\r\n","\r\n","\r\n","@keras_export('keras.applications.vgg16.decode_predictions')\r\n","def decode_predictions(preds, top=5):\r\n","  return imagenet_utils.decode_predictions(preds, top=top)\r\n","\r\n","\r\n","preprocess_input.__doc__ = imagenet_utils.PREPROCESS_INPUT_DOC.format(\r\n","    mode='',\r\n","    ret=imagenet_utils.PREPROCESS_INPUT_RET_DOC_CAFFE,\r\n","    error=imagenet_utils.PREPROCESS_INPUT_ERROR_DOC)\r\n","decode_predictions.__doc__ = imagenet_utils.decode_predictions.__doc__"],"execution_count":null,"outputs":[]}]}
{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Pytorch Tutorial(Autograd, MLP).ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q_lGKmGGqnTa"
      },
      "source": [
        "# Pytorch Tutoral - Autograd & MLP(Multi-layer perceptron)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4WO0S9d_uHX6"
      },
      "source": [
        "# Autograd\n",
        "\n",
        "- `autograd`패키지는 텐서의 모든 연산에 대한 자동 미분을 제공\n",
        "- 실행-기반-정의(define-by-run) 프레임워크로, 코드를 어떻게 작성하여 실행하느냐에 따라 역전파가 정의된다는 것을 의미\n",
        "- 역전파는 학습 과정의 매 단계마다 달라짐\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SF17xKuauw9r"
      },
      "source": [
        "## Tensor\n",
        "\n",
        "- `torch.Tensor` 클래스의 `.requires_grad` 속성을 `True`로 설정하면, 해당 텐서에서 이루어진 모든 연산을 추적(track)하기 시작\n",
        "- 계산이 완료된 후 `.backward()`를 호출하여 모든 변화도(gradient)를 자동으로 계산할 수 있으며 이 Tensor의 변화도는 `.grad` 속성에 누적됨\n",
        "- Tensor가 기록을 추적하는 것을 중단하게 하려면, `.detach()`를 호출하여 연산기록으로부터 분리하여 연산이 추적되는 것을 방지할 수 있음\n",
        "- 기록을 추적하는 것(과 메모리를 사용하는 것)을 방지하기 위해서 코드 블럭을 `with torch.no_grad():`로 감쌀 수 있음\n",
        "- 이는 변화도(gradient)는 필요 없지만 `requires_grad=True`가 설정되어 학습 가능한 매개변수를 갖는 모델을 평가(evaluate)할 때 유용\n",
        "- Autograd 구현에서 `Function` 클래스는 매우 중요한 역할을 수행\n",
        "- `Tensor`와 `Function`은 서로 연결되어 있고 모든 연산 과정을 부호화하여 순환하지 않는 그래프를 생성\n",
        "- 각 tensor는 `.grad_fn` 속성을 가지고 있는데 이는 `Tensor`를 생성한 `Function`을 참조함(단, 사용자가 만든 Tensor는 예외이며, 사용자가 만든 Tensor가 아닌 연산에 의해 생긴 텐서와 같은 경우는 모두 `Function`을 참조)\n",
        "- 도함수를 계산하기 위해서는 `Tensor`의 `.backward()`를 호출하면 됨"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gTKjPjHGtsW1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "90b19fda-ea20-45a3-b315-534df70f1781"
      },
      "source": [
        "import torch\n",
        "\n",
        "print(torch.__version__)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1.7.0+cu101\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Fvs-s2KpPp-h",
        "outputId": "38378dcc-2228-4a2a-a592-5cec9d2ce21e"
      },
      "source": [
        "# x의 연산 과적을 추적하기 위해 requires_grad=True로 설정\n",
        "x = torch.ones(2, 2, requires_grad=True)\n",
        "print(x)\n",
        "\n",
        "# 직접 생선한 Tensor이기 때문에 grad_fn이 None인 것을 확인할 수 있음\n",
        "print(x.grad_fn)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([[1., 1.],\n",
            "        [1., 1.]], requires_grad=True)\n",
            "None\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jaqxHcfkPvQP",
        "outputId": "38558535-512a-4cdb-8b05-ea87db509b46"
      },
      "source": [
        "# y는 연산의 결과로 생성된 것이기 때문에 grad_fn을 갖고 있는 것을 확인 가능\n",
        "y = x + 2\n",
        "print(y)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([[3., 3.],\n",
            "        [3., 3.]], grad_fn=<AddBackward0>)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lw-YLFNBQoqa",
        "outputId": "6cd6453c-23cd-464d-91ef-5cc37a665ef9"
      },
      "source": [
        "# 연산의 결과로 생성된 것이기 때문에 grad_fn을 갖는 것을 확인 가능\n",
        "print(y.grad_fn)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<AddBackward0 object at 0x7efe58ead828>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Im5ImouqdyZd",
        "outputId": "7859207c-73c4-47f5-afad-a0ca86dfdd8e"
      },
      "source": [
        "z = y * y * 3\n",
        "out = z.mean()\n",
        "\n",
        "# 각각 사용한 func에 맞게 grad_fn이 생성된 것을 확인할 수 있음\n",
        "print(z)\n",
        "print(out)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([[27., 27.],\n",
            "        [27., 27.]], grad_fn=<MulBackward0>)\n",
            "tensor(27., grad_fn=<MeanBackward0>)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TF5JzkVjemZl"
      },
      "source": [
        "- `requires_grad_()`를 사용하면 기존 Tensor의 `requires_grad` 값을 바꿀 수 있음\n",
        "- 입력 값이 지정되지 않으면 기본 값은 `False`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0NJ9HewEd_3_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bc8366af-0703-495e-8074-9cf314627fa4"
      },
      "source": [
        "a = torch.randn(2, 2)\n",
        "print(a)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([[-0.7437, -0.3589],\n",
            "        [-2.0437, -2.0806]])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pcnqPfCaezpt",
        "outputId": "753a6d76-d926-4de8-d122-ace7ebad889b"
      },
      "source": [
        "a = ((a * 3) / (a - 1))\n",
        "print(a)\n",
        "print(a.requires_grad)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([[1.2795, 0.7924],\n",
            "        [2.0144, 2.0262]])\n",
            "False\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ItC00jyde5_2",
        "outputId": "36117d85-f005-4017-bc13-a9234622da82"
      },
      "source": [
        "a.requires_grad_(True)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[1.2795, 0.7924],\n",
              "        [2.0144, 2.0262]], requires_grad=True)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mnzFfYAOe7p_",
        "outputId": "7b1f8143-6b84-4748-e9f7-79d93b95d4c0"
      },
      "source": [
        "print(a.requires_grad)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "True\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fzRsVdI6e9G_",
        "outputId": "ca29930e-03a1-4bb2-b964-18eb14e6d30a"
      },
      "source": [
        "b = (a * a).sum()\n",
        "print(b)\n",
        "print(b.requires_grad)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor(10.4281, grad_fn=<SumBackward0>)\n",
            "True\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PvvApXb8fEwT"
      },
      "source": [
        "## 변화도(Gradient)\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 511
        },
        "id": "IFs3cy6DJIBz",
        "outputId": "51718146-6ba8-4345-d9aa-69b4145a7918"
      },
      "source": [
        "print(out)\n",
        "\n",
        "# 이전에 만든 out을 사용해서 역전파 진행\n",
        "\n",
        "y.retain_grad()     # 중간 값에 대한 미분 값을 보고싶다면 해당 값에 대한 retain_grad()를 호출해야 함\n",
        "z.retain_grad()\n",
        "out.backward()      # 여러 번 미분을 진행하기 위해서는 retain_graph=True로 설정해줘야 함(그렇지 않으면 아래처럼 에러 발생)\n",
        "\n",
        "# out.backward(torch.tensor(1.))을 진행하는 것과 동일\n",
        "print(x.grad)\n",
        "print(y.grad)\n",
        "print(z.grad)\n",
        "print(z.is_leaf)\n",
        "\n",
        "out.backward()\n",
        "print(x.grad)\n",
        "print(y.grad)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor(27., grad_fn=<MeanBackward0>)\n",
            "tensor([[4.5000, 4.5000],\n",
            "        [4.5000, 4.5000]])\n",
            "tensor([[4.5000, 4.5000],\n",
            "        [4.5000, 4.5000]])\n",
            "tensor([[0.2500, 0.2500],\n",
            "        [0.2500, 0.2500]])\n",
            "False\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-11-8c63e313f311>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mz\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_leaf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m \u001b[0mout\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m    219\u001b[0m                 \u001b[0mretain_graph\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    220\u001b[0m                 create_graph=create_graph)\n\u001b[0;32m--> 221\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    222\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    223\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m    130\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m    131\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 132\u001b[0;31m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m    133\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    134\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: Trying to backward through the graph a second time, but the saved intermediate results have already been freed. Specify retain_graph=True when calling backward the first time."
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GNPynGc7JRTN",
        "outputId": "074007cf-06a3-4c26-b646-f540772af673"
      },
      "source": [
        "x = torch.ones(2, 2, requires_grad=True)\n",
        "y = x + 2\n",
        "z = y * y * 3\n",
        "out = z.mean()\n",
        "\n",
        "print(out)\n",
        "\n",
        "y.retain_grad()\n",
        "out.backward(retain_graph=True)\n",
        "\n",
        "print(x.grad)\n",
        "print(y.grad)\n",
        "print(z.grad)       # z.retain_grad()를 호출하지 않으면 grad값을 저장하지 않기 때문에 grad 속성을 볼 수 없음\n",
        "print(z.is_leaf)\n",
        "\n",
        "out.backward()\n",
        "print(x.grad)\n",
        "print(y.grad)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor(27., grad_fn=<MeanBackward0>)\n",
            "tensor([[4.5000, 4.5000],\n",
            "        [4.5000, 4.5000]])\n",
            "tensor([[4.5000, 4.5000],\n",
            "        [4.5000, 4.5000]])\n",
            "None\n",
            "False\n",
            "tensor([[9., 9.],\n",
            "        [9., 9.]])\n",
            "tensor([[9., 9.],\n",
            "        [9., 9.]])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:13: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
            "  del sys.path[0]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aEfWBA_IhY6V"
      },
      "source": [
        "$$ out = \\frac{1}{4} \\sum_{i} z_i $$\n",
        "$$ z_i = 3(x_i + 2)^2 $$\n",
        "- `z_i`를 미분한 값에 적용하면 4.5를 동일하게 출력할 수 있음\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jYSGfXrMhU9S"
      },
      "source": [
        "out(미분결과) = 1/4 * 6 * (x + 2) = 3/2(x+2) "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o0XT_UAPj01j"
      },
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EUbsQZvyjPw6"
      },
      "source": [
        "- 일반적으로 `torch.autograd`는 벡터-야코비안 곱을 계산하는 엔진\n",
        "- `torch.autograd`를 사용하면 전체 야코비안을 직접 계산할 수는 없지만, 벡터-야코비안 곱은 `backward`에 해당 벡터를 인자로 제공하여 얻을 수 있음"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6i9NlL6NhBxo"
      },
      "source": [
        "x = torch.randn(3, requires_grad=True)\n",
        "\n",
        "y = x * 2\n",
        "\n",
        "while y.data.norm() < 1000:\n",
        "  y = y * 2\n",
        "\n",
        "print(y)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GQM3R2CAkEvw"
      },
      "source": [
        "# scalar값이 아닌 y의 벡터-야코비안 곱을 구하는 과정\n",
        "v = torch.tensor([0.1, 1.0, 0.0001], dtype=torch.float)\n",
        "y.backward(v)\n",
        "\n",
        "print(x.grad)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HOpIFRDiknBR"
      },
      "source": [
        "- `with torch.no_grad()`로 코드 블록을 감싸서 `autograd`가 `.requires_grad=True`인 Tensor의 연산 기록을 추적하는 것을 멈출 수도 있음"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pVcZI7_ykkBy"
      },
      "source": [
        "print(x.requires_grad)\n",
        "print((x ** 2).requires_grad)\n",
        "\n",
        "with torch.no_grad():\n",
        "  print((x ** 2).requires_grad)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "64f5Z32tk5Kd"
      },
      "source": [
        "- 또는 `.detach()`를 호출하여 내용물은 같지만 `requires_grad`가 다른 새로운 텐서를 가져올 수 있음"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kpct8uBPk3st"
      },
      "source": [
        "print(x.requires_grad)\n",
        "y = x.detach()\n",
        "print(y.requires_grad)\n",
        "print(x.eq(y).all())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kw0wgV63lLfa"
      },
      "source": [
        "# ANN(Artificial Neural Networks)\n",
        "\n",
        "- 신경망은 `torch.nn` 패키지를 사용하여 생성할 수 있음\n",
        "- `nn`은 모델을 정의하고 미분하기 위해서 위에서 살펴본 `autograd`를 사용\n",
        "- `nn.Module`은 계층(layer)과 `output`을 반환하는 `forward(input)` 메소드를 포함"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K1CPO19LwmAI"
      },
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UIVy4DXNmqEN"
      },
      "source": [
        "- 간단한 순전파 네트워크(feed-forward-network)\n",
        "- 입력을 받아 여러 계층에 차례로 전달한 후, 최종 출력을 제공\n",
        "- 신경망의 일반적인 학습 과정\n",
        "\n",
        "\n",
        "> - 학습 가능한 매개변수(가중치)를 갖는 신경망을 정의\n",
        "> - 데이터 셋 입력을 반복\n",
        "> - 입력을 신경망에서 전파(process)\n",
        "> - 손실(loss; 입력 값과 예측 값과의 차이)를 계산\n",
        "> - 변화도(gradient)를 신경망의 매개변수들에 역으로 전파 - 역전파 과정\n",
        "> - 신경망의 가중치를 갱신\n",
        "> > - `새로운 가중치(weight) = 가중치(weight) - 학습률(learning rate) * 변화도(gradient)`\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lSK4LCt5lGiJ"
      },
      "source": [
        "import pandas as pd\n",
        "\n",
        "from sklearn.datasets import load_iris\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import DataLoader, TensorDataset"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q-G_wst4GaFd"
      },
      "source": [
        "class Net(nn.Module):\n",
        "\n",
        "    def __init__(self):\n",
        "        super(Net, self).__init__()\n",
        "\n",
        "        self.layer0 = nn.Linear(4, 128)\n",
        "        self.layer1 = nn.Linear(128, 64)\n",
        "        self.layer2 = nn.Linear(64, 32)\n",
        "        self.layer3 = nn.Linear(32, 16)\n",
        "        self.layer4 = nn.Linear(16, 3)\n",
        "\n",
        "        self.bn0 = nn.BatchNorm1d(128)\n",
        "        self.bn1 = nn.BatchNorm1d(64)\n",
        "        self.bn2 = nn.BatchNorm1d(32)\n",
        "\n",
        "        self.act = nn.ReLU()\n",
        "\n",
        "    def forward(self, x):\n",
        "      x = self.act(self.bn0(self.layer0(x)))\n",
        "      x = self.act(self.bn1(self.layer1(x)))\n",
        "      x = self.act(self.bn2(self.layer2(x)))\n",
        "      x = self.act(self.layer3(x))\n",
        "      x = self.layer4(x)\n",
        "\n",
        "      return x"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1iriyhtY-wZy"
      },
      "source": [
        "## 손실 함수(Loss Function)\n",
        "\n",
        "- 손실 함수는 (output, target)을 한 쌍으로 입력 받아, 출력이 정답으로부터 얼마나 떨어져있는지 추정하는 값을 계산\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OZ8bodFU5R0q"
      },
      "source": [
        "- `forward`함수만 정의하고 나면 `backward`함수는 `autograd`를 사용하여 자동으로 정의됨\n",
        "- 모델의 학습 가능한 매개 변수는 `net.parameters()`에 의해 변환됨"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RFQQ1Hd9-wDv"
      },
      "source": [
        "# 랜덤 값 생성\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "ex_X, ex_y = torch.randn([4, 4]), torch.tensor([1, 0, 2, 0])\n",
        "\n",
        "net = Net()\n",
        "output = net(ex_X)\n",
        "loss = criterion(output, ex_y)\n",
        "print('loss: ', loss.item())\n",
        "  \n",
        "net.zero_grad()\n",
        "\n",
        "print('layer0.bias.grad before backward')\n",
        "print(net.layer4.bias.grad)\n",
        "\n",
        "print(net.layer4.bias.is_leaf)\n",
        "\n",
        "loss.backward()\n",
        "\n",
        "print('layer0.bias.grad after backward')\n",
        "print(net.layer4.bias.grad)\n",
        "\n",
        "\"\"\"\n",
        "이 부분에서 .retain_grad() 를 사용하지 않아도 되는 이유는 weight와 bias의 파라미터가 leaf 노드이기 때문이라고 합니다.\n",
        "설명이 포함된 링크들을 아래에 첨부해두니 읽어보시면 좋을 것 같습니다 :)\n",
        "\n",
        "어떤 경우가 leaf 노드 인지 - https://discuss.pytorch.org/t/leaf-variable-was-used-in-an-inplace-operation/308/20\n",
        "왜 leaf 노드인 경우에는 retain_grad()를 하지 않아도 grad 값이 나오는지 - https://discuss.pytorch.org/t/what-is-the-purpose-of-is-leaf/87000/7\n",
        "\"\"\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tJ8Re3qL-ICi"
      },
      "source": [
        "params = list(net.parameters())\n",
        "print(len(params))\n",
        "print(params[0].size())  # layer0의 weight"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "stc5MlORHVss"
      },
      "source": [
        "## 가중치 갱신\n",
        "- 가장 단순한 갱신 규칙은 확률적 경사하강법(SGD; Stochastic Gradient Descent)\n",
        "- `새로운 가중치(weight) = 가중치(weight) - 학습률(learning rate) * 변화도(gradient)`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "do674u-5HYdI"
      },
      "source": [
        "# torch.optim 패키지에 다양한 갱신 규칙이 규현되어 있음\n",
        "\n",
        "import torch.optim as optim\n",
        "\n",
        "optimizer = optim.SGD(net.parameters(), lr=0.001)\n",
        "\n",
        "optimizer.zero_grad()\n",
        "output = net(ex_X)\n",
        "loss = criterion(output, ex_y)\n",
        "loss.backward()\n",
        "optimizer.step()  # 업데이트 진행"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bS5ZVLPDnVYN"
      },
      "source": [
        "## MLP 모델"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xyLsrD2AoMsz"
      },
      "source": [
        "dataset = load_iris()\n",
        "\n",
        "data = dataset.data\n",
        "label = dataset.target\n",
        "\n",
        "print(dataset.DESCR)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G8loKlfN0JOc"
      },
      "source": [
        "print('shape of data: ', data.shape)\n",
        "print('shape of label: ',label.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y3xw0IFk0XjL"
      },
      "source": [
        "# 훈련과 테스트 데이터로 나누기\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(data, label, test_size=0.25)\n",
        "print(len(X_train))\n",
        "print(len(X_test))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qiHojNiZ0XgW"
      },
      "source": [
        "# DataLoader 생성\n",
        "X_train = torch.from_numpy(X_train).float()\n",
        "y_train = torch.from_numpy(y_train).long()\n",
        "\n",
        "X_test = torch.from_numpy(X_test).float()\n",
        "y_test = torch.from_numpy(y_test).long()\n",
        "\n",
        "train_set = TensorDataset(X_train, y_train)\n",
        "\n",
        "train_loader = DataLoader(train_set, batch_size=4, shuffle=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RKMCSkmw0XdW"
      },
      "source": [
        "class Net(nn.Module):\n",
        "\n",
        "    def __init__(self):\n",
        "        super(Net, self).__init__()\n",
        "\n",
        "        self.layer0 = nn.Linear(4, 128)\n",
        "        self.layer1 = nn.Linear(128, 64)\n",
        "        self.layer2 = nn.Linear(64, 32)\n",
        "        self.layer3 = nn.Linear(32, 16)\n",
        "        self.layer4 = nn.Linear(16, 3)\n",
        "\n",
        "        self.bn0 = nn.BatchNorm1d(128)\n",
        "        self.bn1 = nn.BatchNorm1d(64)\n",
        "        self.bn2 = nn.BatchNorm1d(32)\n",
        "\n",
        "        self.act = nn.ReLU()\n",
        "\n",
        "    def forward(self, x):\n",
        "      x = self.act(self.bn0(self.layer0(x)))\n",
        "      x = self.act(self.bn1(self.layer1(x)))\n",
        "      x = self.act(self.bn2(self.layer2(x)))\n",
        "      x = self.act(self.layer3(x))\n",
        "      x = self.layer4(x)\n",
        "\n",
        "      return x\n",
        "      # return nn.Softmax(x)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_JwxsEnD0XWK"
      },
      "source": [
        "net = Net()\n",
        "print(net)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tTzItXLv24HR"
      },
      "source": [
        "optimizer = torch.optim.SGD(net.parameters(), lr=0.001)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "epochs = 200"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s7TSeMPP3TwB"
      },
      "source": [
        "losses = list()\n",
        "accuracies = list()\n",
        "\n",
        "for epoch in range(epochs):\n",
        "  epoch_loss = 0  \n",
        "  epoch_accuracy = 0\n",
        "  for X, y in train_loader:\n",
        "  \n",
        "    optimizer.zero_grad()\n",
        "    \n",
        "    output = net(X)\n",
        "\n",
        "    loss = criterion(output, y)\n",
        "    loss.backward()\n",
        "    \n",
        "    optimizer.step()\n",
        "    # output = [0.11, 0.5, 0.8]  --> 예측 클래스 값\n",
        "    _, predicted = torch.max(output, dim=1)\n",
        "    accuracy = (predicted == y).sum().item()\n",
        "    epoch_loss += loss.item()\n",
        "    epoch_accuracy += accuracy\n",
        "  \n",
        "\n",
        "  epoch_loss /= len(train_loader)\n",
        "  epoch_accuracy /= len(X_train)\n",
        "  print(\"epoch :{}, \\tloss :{}, \\taccuracy :{}\".format(str(epoch+1).zfill(3),round(epoch_loss,4), round(epoch_accuracy,4)))\n",
        "  \n",
        "  losses.append(epoch_loss)\n",
        "  accuracies.append(epoch_accuracy)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jyD5qnrK36oE"
      },
      "source": [
        "# Plot result\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.figure(figsize=(20,5))\n",
        "plt.subplots_adjust(wspace=0.2)\n",
        "\n",
        "plt.subplot(1,2,1)\n",
        "plt.title(\"$loss$\",fontsize = 18)\n",
        "plt.plot(losses)\n",
        "plt.grid()\n",
        "plt.xlabel(\"$epochs$\", fontsize = 16)\n",
        "plt.xticks(fontsize = 14)\n",
        "plt.yticks(fontsize = 14)\n",
        "\n",
        "\n",
        "plt.subplot(1,2,2)\n",
        "plt.title(\"$accuracy$\", fontsize = 18)\n",
        "plt.plot(accuracies)\n",
        "plt.grid()\n",
        "plt.xlabel(\"$epochs$\", fontsize = 16)\n",
        "plt.xticks(fontsize = 14)\n",
        "plt.yticks(fontsize = 14)\n",
        "\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RY6hOROnnaRG"
      },
      "source": [
        "# Test\n",
        "\n",
        "output = net(X_test)\n",
        "print(torch.max(output, dim=1))\n",
        "_, predicted = torch.max(output, dim=1)\n",
        "accuracy = round((predicted == y_test).sum().item() / len(y_test),4)\n",
        "\n",
        "\n",
        "print(\"test_set accuracy :\", round(accuracy,4))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MeVLbpX342HJ"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}